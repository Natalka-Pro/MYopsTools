# [MLOps](https://github.com/girafe-ai/mlops)

## Инструкция по запуску пакета в чистом окружении:
```bash
git https://github.com/Natalka-Pro/myops_tools.git
cd myops_tools/
```

```bash
virtualenv -p /usr/bin/python3.11 venv_name
source venv_name/bin/activate
```

```bash
poetry install
pre-commit install
pre-commit run -a
```

Запуск сервера mlflow (mlruns):
```bash
mlflow server --host 127.0.0.1 --port 8080
```

Запуск сервера triton:
```bash
cd triton/
docker-compose up
```

Команды модели:
```bash
python commands.py train
python commands.py infer
python commands.py run_server
python commands.py client
```

## Описание ML задачи
Классификация рукописных цифр - датасет [torchvision.datasets.MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html).

60000 изображений для обучения и 10000 изображений для тестирования.

Задача решается очень упрощённой свёрточной нейронной сетью AlexNet.


---
# Первая домашняя работа
**Текст задания:**

Необходимо создать открытый репозиторий, в нём создать пакет на Python, который будет являться валидным с точки зрения пакетирования и решать какую-либо задачу машинного обучения (сейчас не так важно какую, для примера можете взять [классификацию кошек против собак](https://github.com/girafe-ai/ml-course/blob/master/week0_10_cnn/week10_cnn_seminar.ipynb) из нашего курса по МЛ). Выбирайте задачу, которая у вас будет обучаться не более 5 минут!!! Иначе и вы, и мы будем долго ждать.

Под "решать" понимается дву вещи:
1. есть файл train.py, в котором данные загружаются, модель тренируется и сохраняется на диск
2. есть файл infer.py, который считывает с диска модель из предыдущего пункта, загружает валидационный датасет, предсказывает моделью ответы для этих данных, записывает ответы на диск в .csv файл, выводит в stdout (`print`) необходимые метрики на этом датасете.

Как мы будем проверять ДЗ:
1. клонируем репозиторий
2. создаём новый чистый virtualenv
3. poetry install - нужна успешная установка (верная конфигурация)
4. pre-commit install - нужна успешная установка (верная конфигурация)
5. pre-commit run -a - не должно быть проваленных хуков (необходимые хуки это black, isort, flake8)
6. python train.py - ожидаем сохранённую модель
7. python infer.py - ожидаем печать метрик и файл с ответами

Технические подсказки:
1. Не забывайте про if \__name__ == '\__main__':
2. Пишите в вашу дефолтную ветку, мы не будем переключать, проверка скриптованная.

**Дедлайн (жёсткий): суббота, 7 октября, 12.00 по Мск**


---
# Вторая домашняя работа
**Текст задания:**

Следующий этап развития проекта это добавление системы управления данными, в нашем случае это dvc, конфигурации экспериментов на основе hydra, а также логирование на основе mlflow. Для тех, кто делает проекты с нейросетями будет плюсом использование фреймворка для обучения модели (catalyst, lightning, hugging face). Напоследок сконфигурировать простой сервер предсказаний модели используя mlflow models.

1. DVC
Для dvc в качестве бекенда проще и доступнее всего использовать гугл диск (проверьте, что папка доступна по ссылке на чтение всем, иначе мы не сможем проверить), можно использовать и любой другой бекенд, но тут возникает такой же вопрос с доступностью.
Скачивание данных с помощью dvc необходимо встроить в имеющиеся команды train и infer, для этого у dvc есть python api (на крайний случай можно дёрнуть CLI).

2. Hydra
Переведите основные гиперпараметры препроцессинга, обучения и постпроцессинга в yaml конфиги hydra. Сами конфиги лучше всего расположить в папке configs в корне репозитория.

3. Logging
Необходимо добавить логирование ваших основных метрик и функций потерь (всего не менее 3 графиков). Также в эксперимент записывать использованные гиперпараметры и версию кода (git commit id). Считайте, что сервер mlflow уже поднят, его адрес добавьте в поле конфига.

4. Inference
Экспортируйте модель в onnx и добавьте команду run_server (либо в fire, либо отдельным .py файлом), в которой используя mlflow models запустите инференс модели из onnx файла.

Результат работы оформите в виде тега с названием ‘hw2’ в git репозитории.

**Дедлайн (жёсткий): вскр, 3 декабря, 23.59 по Мск**

Репозиторий у вас остаётся прежний, поэтому формы заполнять не нужно, достаточно создать тег в git репозитории. Для тех, кто ещё не отправлял нам адрес репы - это можно сделать [в той же форме](https://docs.google.com/forms/d/e/1FAIpQLSfLlurK3Z3KQNnobvvKD8ObZ5NVdaiwf57jaVVHnHoautivpw/viewform?usp=sf_link)


---
# Третья домашняя работа
**Текст задания:**

Следующий этап - оптимизация деплоя модели. Для успешного выполнения этого задания вам потребуется только докер. Основная часть - поднять Nvidia Triton Server с вашей моделью/моделями. Две доп части: одна на TensorRT (потребуется гпу), вторая теоретическая (ничего сверху того что есть не потребуется).

1. Определитесь с выбором бекенда:
- ONNX - если в HW2 модель экспортировалась в этот формат
- FIL (Forest Inference LIbrary) - будет плюсом если у вас что-то из  XGBoost, LightGBM, Scikit-Learn, cuML
- Python - если модель не подходит ни в один из форматов a, b
P.S. Если у вас onnx модель, но вы вставите ее в Python бекенд, то это минус.

2. При помощи SDK утилиты perf_analyzer найдите оптимальные параметры для config.pbtxt (варианты на что стоит обратить внимание, мы обсуждали на 9ом семинаре, еще можно почитать [тут](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/optimization.md)).
Запишите в .md отчете мотивацию выбора тех или иных параметров (про формат отчета будет ниже). Минимум в этом пункте - настроить dynamic batching и instance groups

3. Напишите клиента для работы с тритоном: функции запаковки и распаковки, а так же оформите несколько тестов. Входные и ожидаемые значения можно захардкодить

4. Форма текстового отчета:
- Ваша системная конфигурация
    - OS и версия
    - Модель CPU
    - Количество vCPU и RAM при котором собирались метрики
- Описание решаемой задачи
- Описание структуры вашего model_repository (в формате “$ tree”)
- Секция с метриками по throughput и latency которые вы замерили до всех оптимизаций и после всех оптимизаций
- Объяснение мотивации выбора или не выбора той или иной оптимизации

5. Что точно должно быть в коде:
- Скрипт для конвертации вашей модели в формат выбранного бекенда
- Дерево model_repository (P.S. кладите .gitkeep в директории с весами моделей и добавляйте в .gitignore пути до весов)
- Тесты для задеплоенной модели (P.S. не делайте случайные тесты, они должны быть воспроизводимыми)

6. Что точно должно быть в dvc:
- Веса моделей

**На доп плюсик:** Если у вас есть гпу, то можете поэксперементировать с конвертацией в tensorrt. Добавьте в отчет команду для конвертации и объясните набор флагов который выставите в trtexec (желательно не только мотивация попавших флагов, но так же объяснение отсутствующих флагов). Так же добавьте замеры throughput и latency для trt модели, сравните с метриками для onnx. В таком случае ОБЯЗАТЕЛЬНО: в итоговом model_repository должна быть как onnx модель так и trt модель. Клиента пишите под onnx модель (так как вероятно мы будем запускать в окружении без гпу).

**На еще один доп плюсик:**
Если модель может выполняться на гпу (теоретически), то выясните для каждого слоя является ли он ограниченным арифметикой либо ограниченным памятью (соответствующие темы разбирались на 7-8 лекциях). Для этого выясните количество флопс в каждом слое модели (можно автоматизировать через библиотеку [thop](https://github.com/Lyken17/pytorch-OpCounter/)) и количество байт памяти для выполнения forward pass слоя, затем переведите в ```#ops/#bytes``` и сравните с пиковыми ```#ops/#bytes``` для вашей видеокарты и сделайте вывод. Попробуйте подобрать оптимальный размер батча для достижения максмальной арифметической интенсивности для вашей видеокарты. Все это (+ модель вашей видеокарты) занесите в текстовый отчет.

**Дедлайн (жёсткий): 23 декабря 23:59**
